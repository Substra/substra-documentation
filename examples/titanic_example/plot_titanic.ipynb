{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Titanic\n",
        "\n",
        "\n",
        "This example is based on `the similarly named Kaggle challenge <https://www.kaggle.com/c/titanic/overview>`__.\n",
        "\n",
        "In this example, we work on the Titanic tabular dataset. The problem considered is a classification problem\n",
        "and the model used is a random forest model.\n",
        "\n",
        "Here you will learn how to interact with Substra including:\n",
        "\n",
        "- instantiating Substra Client\n",
        "- creating and registering of the assets\n",
        "- launching an experiment\n",
        "\n",
        "\n",
        "There is no federated learning in this example, training and testing will happen on only one :term:`Organization`.\n",
        "\n",
        "Requirements:\n",
        "\n",
        "  - If you want to run this example locally please make sure to download and unzip in the same directory as this example\n",
        "    the assets needed to run it:\n",
        "\n",
        "    .. only:: builder_html or readthedocs\n",
        "\n",
        "        :download:`assets required to run this example <../../../../tmp/titanic_assets.zip>`\n",
        "\n",
        "    Please ensure you have all the libraries in this file installed, the requirements.txt file is included in this zip, you can pip install it with a command: `pip install -r requirements.txt`.\n",
        "\n",
        "  - Substra should already be installed, if not follow the instructions described here: `substrafl_doc/substrafl_overview:Installation`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import all the dependencies\n",
        "---------------------------\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "import substra\n",
        "from substra.sdk.schemas import (\n",
        "    AlgoSpec,\n",
        "    AlgoCategory,\n",
        "    AlgoInputSpec,\n",
        "    AlgoOutputSpec,\n",
        "    AssetKind,\n",
        "    DataSampleSpec,\n",
        "    DatasetSpec,\n",
        "    Permissions,\n",
        "    TesttupleSpec,\n",
        "    PredicttupleSpec,\n",
        "    TraintupleSpec,\n",
        "    ComputeTaskOutputSpec,\n",
        "    InputRef,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Instantiating the Substra Client\n",
        "================================\n",
        "\n",
        "The client allows us to interact with the Substra platform. Setting the debug argument to ``True`` allow us to work locally by emulating a platform.\n",
        "\n",
        "By setting the argument ``backend_type`` to:\n",
        "\n",
        " - ``docker`` all tasks will be executed from docker containers (default)\n",
        " - ``subprocess`` all tasks will be executed from Python subprocesses (faster)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "client = substra.Client(backend_type=\"subprocess\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creation and Registration of the assets\n",
        "---------------------------------------\n",
        "\n",
        "Every asset will be created respecting its respective predefined schemas (Spec) previously imported from\n",
        "substra.sdk.schemas. To register assets, first assets `documentation/api_reference:Schemas`\n",
        "are instantiated and then the specs are registered, which creates the real assets.\n",
        "\n",
        "Permissions are defined when registering assets, in a nutshell:\n",
        "\n",
        "- data can not be seen once it's registered on the platform\n",
        "- metadata are visible by all the users of a channel\n",
        "- permissions are permissions to execute an algorithm on a certain dataset.\n",
        "\n",
        "On a remote deployment setting the parameter ``public`` to false means that the dataset can only be used by tasks in\n",
        "the same organization or organizations that are in the ``authorized_ids``. However permissions are ignored in local mode.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "permissions = Permissions(public=True, authorized_ids=[])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we need to define the asset directory. You should have already downloaded the assets folder as stated above.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "root_dir = Path.cwd()\n",
        "assets_directory = root_dir / \"assets\"\n",
        "assert assets_directory.is_dir(), \"\"\"Did not find the asset directory, a directory called 'assets' is\n",
        "expected in the same location as this py file\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Registering data samples and dataset\n",
        "====================================\n",
        "\n",
        "A dataset represents the data in Substra. It is made up of an opener, which is a script used to load the\n",
        "data from files into memory. You can find more details about the Dataset\n",
        "in the API reference.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dataset = DatasetSpec(\n",
        "    name=\"Titanic dataset - Org 1\",\n",
        "    type=\"csv\",\n",
        "    data_opener=assets_directory / \"dataset\" / \"titanic_opener.py\",\n",
        "    description=assets_directory / \"dataset\" / \"description.md\",\n",
        "    permissions=permissions,\n",
        "    logs_permission=permissions,\n",
        ")\n",
        "\n",
        "dataset_key = client.add_dataset(dataset)\n",
        "print(f\"Dataset key {dataset_key}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Adding train data samples\n",
        "=========================\n",
        "\n",
        "The dataset object itself is an empty shell, to add actual data, data samples are needed.\n",
        "A data sample contains subfolders containing a single data file like a CSV and the key identifying\n",
        "the dataset it is linked to.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train_data_sample_folder = assets_directory / \"train_data_samples\"\n",
        "train_data_sample_keys = client.add_data_samples(\n",
        "    DataSampleSpec(\n",
        "        paths=list(train_data_sample_folder.glob(\"*\")),\n",
        "        test_only=False,\n",
        "        data_manager_keys=[dataset_key],\n",
        "    )\n",
        ")\n",
        "\n",
        "print(f\"{len(train_data_sample_keys)} data samples were registered\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Adding test data samples\n",
        "========================\n",
        "The operation is done again but with the test data samples.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "test_data_sample_folder = assets_directory / \"test_data_samples\"\n",
        "test_data_sample_keys = client.add_data_samples(\n",
        "    DataSampleSpec(\n",
        "        paths=list(test_data_sample_folder.glob(\"*\")),\n",
        "        test_only=True,\n",
        "        data_manager_keys=[dataset_key],\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(f\"{len(test_data_sample_keys)} data samples were registered\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The data has now been added as an asset through the datasamples both for the training and\n",
        "testing part of our experience.\n",
        "\n",
        "Adding Metrics\n",
        "==============\n",
        "A metric corresponds to a function to evaluate the performance of a model on a dataset.\n",
        "Concretely, a metric corresponds to an archive (tar or zip file) containing:\n",
        "\n",
        "- Python scripts that implement the metric computation\n",
        "- a Dockerfile on which the user can specify the required dependencies of the Python scripts\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "inputs_metrics = [\n",
        "    AlgoInputSpec(identifier=\"datasamples\", kind=AssetKind.data_sample, optional=False, multiple=True),\n",
        "    AlgoInputSpec(identifier=\"opener\", kind=AssetKind.data_manager, optional=False, multiple=False),\n",
        "    AlgoInputSpec(identifier=\"predictions\", kind=AssetKind.model, optional=False, multiple=False),\n",
        "]\n",
        "\n",
        "outputs_metrics = [AlgoOutputSpec(identifier=\"performance\", kind=AssetKind.performance, multiple=False)]\n",
        "\n",
        "\n",
        "METRICS_DOCKERFILE_FILES = [\n",
        "    assets_directory / \"metric\" / \"titanic_metrics.py\",\n",
        "    assets_directory / \"metric\" / \"Dockerfile\",\n",
        "]\n",
        "\n",
        "metric_archive_path = assets_directory / \"metric\" / \"metrics.zip\"\n",
        "\n",
        "with zipfile.ZipFile(metric_archive_path, \"w\") as z:\n",
        "    for filepath in METRICS_DOCKERFILE_FILES:\n",
        "        z.write(filepath, arcname=os.path.basename(filepath))\n",
        "\n",
        "METRICS = AlgoSpec(\n",
        "    inputs=inputs_metrics,\n",
        "    outputs=outputs_metrics,\n",
        "    category=AlgoCategory.metric,\n",
        "    name=\"Accuracy\",\n",
        "    description=assets_directory / \"metric\" / \"description.md\",\n",
        "    file=metric_archive_path,\n",
        "    permissions=permissions,\n",
        ")\n",
        "\n",
        "metric_key = client.add_algo(METRICS)\n",
        "\n",
        "print(f\"Metric key {metric_key}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Adding Algo\n",
        "===========\n",
        "An algorithm specifies the method to train a model on a dataset or the method to aggregate models.\n",
        "Concretely, an algorithm corresponds to an archive (tar or zip file) containing:\n",
        "\n",
        "- One or more Python scripts that implement the algorithm. Importantly, a train and a\n",
        "  predict functions have to be defined.\n",
        "- A Dockerfile on which the user can specify the required dependencies of the Python scripts.\n",
        "  this dockerfile also specifies the method name to execute (either train or predict here)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ALGO_KEYS_JSON_FILENAME = \"algo_random_forest_keys.json\"\n",
        "\n",
        "ALGO_TRAIN_DOCKERFILE_FILES = [\n",
        "    assets_directory / \"algo_random_forest/titanic_algo_rf.py\",\n",
        "    assets_directory / \"algo_random_forest/train/Dockerfile\",\n",
        "]\n",
        "\n",
        "train_archive_path = assets_directory / \"algo_random_forest\" / \"algo_random_forest.zip\"\n",
        "with zipfile.ZipFile(train_archive_path, \"w\") as z:\n",
        "    for filepath in ALGO_TRAIN_DOCKERFILE_FILES:\n",
        "        z.write(filepath, arcname=os.path.basename(filepath))\n",
        "\n",
        "inputs_algo_simple = [\n",
        "    AlgoInputSpec(identifier=\"datasamples\", kind=AssetKind.data_sample, optional=False, multiple=True),\n",
        "    AlgoInputSpec(identifier=\"opener\", kind=AssetKind.data_manager, optional=False, multiple=False),\n",
        "    AlgoInputSpec(identifier=\"models\", kind=AssetKind.model, optional=True, multiple=True),\n",
        "]\n",
        "\n",
        "outputs_algo_simple = [AlgoOutputSpec(identifier=\"model\", kind=AssetKind.model, multiple=False)]\n",
        "\n",
        "TRAIN_ALGO = AlgoSpec(\n",
        "    name=\"Titanic: Random Forest\",\n",
        "    inputs=inputs_algo_simple,\n",
        "    outputs=outputs_algo_simple,\n",
        "    description=assets_directory / \"algo_random_forest\" / \"description.md\",\n",
        "    file=train_archive_path,\n",
        "    permissions=permissions,\n",
        "    category=\"ALGO_SIMPLE\",\n",
        ")\n",
        "\n",
        "\n",
        "train_algo_key = client.add_algo(TRAIN_ALGO)\n",
        "\n",
        "print(f\"Train algo key {train_algo_key}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The predict algo uses the python file as the algo used for training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ALGO_PREDICT_DOCKERFILE_FILES = [\n",
        "    assets_directory / \"algo_random_forest/titanic_algo_rf.py\",\n",
        "    assets_directory / \"algo_random_forest/predict/Dockerfile\",\n",
        "]\n",
        "\n",
        "predict_archive_path = assets_directory / \"algo_random_forest\" / \"algo_random_forest.zip\"\n",
        "with zipfile.ZipFile(predict_archive_path, \"w\") as z:\n",
        "    for filepath in ALGO_PREDICT_DOCKERFILE_FILES:\n",
        "        z.write(filepath, arcname=os.path.basename(filepath))\n",
        "\n",
        "inputs_algo_predict = [\n",
        "    AlgoInputSpec(identifier=\"datasamples\", kind=AssetKind.data_sample, optional=False, multiple=True),\n",
        "    AlgoInputSpec(identifier=\"opener\", kind=AssetKind.data_manager, optional=False, multiple=False),\n",
        "    AlgoInputSpec(identifier=\"models\", kind=AssetKind.model, optional=False, multiple=False),\n",
        "]\n",
        "\n",
        "outputs_algo_predict = [AlgoOutputSpec(identifier=\"predictions\", kind=AssetKind.model, multiple=False)]\n",
        "\n",
        "predict_algo_spec = AlgoSpec(\n",
        "    name=\"Titanic: Random Forest - predict\",\n",
        "    inputs=inputs_algo_predict,\n",
        "    outputs=outputs_algo_predict,\n",
        "    description=assets_directory / \"algo_random_forest\" / \"description.md\",\n",
        "    file=predict_archive_path,\n",
        "    permissions=permissions,\n",
        "    category=\"ALGO_PREDICT\",\n",
        ")\n",
        "\n",
        "predict_algo_key = client.add_algo(predict_algo_spec)\n",
        "\n",
        "print(f\"Predict algo key {predict_algo_key}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The data, the algorithm and the metric are now registered.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Registering tasks\n",
        "-----------------\n",
        "The next step is to register the actual machine learning tasks (or \"tuples\").\n",
        "First a training task is registered which will produce a machine learning model.\n",
        "Then a testing task is registered, testing the model of the training task.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data_manager_input = [InputRef(identifier=\"opener\", asset_key=dataset_key)]\n",
        "train_data_sample_inputs = [InputRef(identifier=\"datasamples\", asset_key=key) for key in train_data_sample_keys]\n",
        "test_data_sample_inputs = [InputRef(identifier=\"datasamples\", asset_key=key) for key in test_data_sample_keys]\n",
        "\n",
        "traintuple = TraintupleSpec(\n",
        "    algo_key=train_algo_key,\n",
        "    data_manager_key=dataset_key,\n",
        "    train_data_sample_keys=train_data_sample_keys,\n",
        "    outputs={\"model\": ComputeTaskOutputSpec(permissions=permissions)},\n",
        "    inputs=data_manager_input + train_data_sample_inputs,\n",
        ")\n",
        "\n",
        "traintuple_key = client.add_traintuple(traintuple)\n",
        "\n",
        "print(f\"Traintuple key {traintuple_key}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In local mode, the registered task is executed at once:\n",
        "the registration function returns a value once the task has been executed.\n",
        "\n",
        "In deployed mode, the registered task is added to a queue and treated asynchronously: this means that the\n",
        "code that registers the tasks keeps executing. To wait for a task to be done, create a loop and get the task\n",
        "every n seconds until its status is done or failed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_input = [InputRef(identifier=\"models\", parent_task_key=traintuple_key, parent_task_output_identifier=\"model\")]\n",
        "\n",
        "predicttuple = PredicttupleSpec(\n",
        "    traintuple_key=traintuple_key,\n",
        "    algo_key=predict_algo_key,\n",
        "    data_manager_key=dataset_key,\n",
        "    test_data_sample_keys=test_data_sample_keys,\n",
        "    outputs={\"predictions\": ComputeTaskOutputSpec(permissions=permissions)},\n",
        "    inputs=data_manager_input + test_data_sample_inputs + model_input,\n",
        ")\n",
        "\n",
        "predicttuple_key = client.add_predicttuple(predicttuple)\n",
        "\n",
        "predictions_input = [\n",
        "    InputRef(identifier=\"predictions\", parent_task_key=predicttuple_key, parent_task_output_identifier=\"predictions\")\n",
        "]\n",
        "\n",
        "testtuple = TesttupleSpec(\n",
        "    algo_key=metric_key,\n",
        "    predicttuple_key=predicttuple_key,\n",
        "    test_data_sample_keys=test_data_sample_keys,\n",
        "    data_manager_key=dataset_key,\n",
        "    outputs={\"performance\": ComputeTaskOutputSpec(permissions=permissions)},\n",
        "    inputs=data_manager_input + test_data_sample_inputs + predictions_input,\n",
        ")\n",
        "\n",
        "testtuple_key = client.add_testtuple(testtuple)\n",
        "\n",
        "print(f\"Testtuple key {testtuple_key}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Results\n",
        "-------\n",
        "Now we can view the results\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "testtuple = client.get_testtuple(testtuple_key)\n",
        "print(testtuple.status)\n",
        "print(\"Metric: \", testtuple.algo.name)\n",
        "print(\"Performance on the metric: \", list(testtuple.test.perfs.values())[0])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8.2 ('substra-doc-3.8')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "0c7d669af69b588d633842e45e95b2ea8cdef6050eb11515cc6144b5802e08fa"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
