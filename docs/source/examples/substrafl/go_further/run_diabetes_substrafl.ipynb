{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Federated Analytics on the diabetes dataset\n",
        "\n",
        "This example demonstrates how to use the flexibility of the SubstraFL library and the base class\n",
        "ComputePlanBuilder to do Federated Analytics. It reproduces the [diabetes example](https://docs.substra.org/en/stable/examples/substra_core/diabetes_example/run_diabetes.html).\n",
        "of the Substra SDK example section using SubstraFL.\n",
        "If you are new to SubstraFL, we recommend to start by the [MNIST Example](https://docs.substra.org/en/stable/examples/substrafl/get_started/run_mnist_torch.htm).\n",
        "to learn how to use the library in the simplest configuration first.\n",
        "\n",
        "We use the **Diabetes dataset** available from the [Scikit-Learn dataset module](https://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset).\n",
        "This dataset contains medical information such as Age, Sex or Blood pressure.\n",
        "The goal of this example is to compute some analytics such as Age mean, Blood pressure standard deviation or Sex percentage.\n",
        "\n",
        "We simulate having two different data organizations, and a third organization which wants to compute aggregated analytics\n",
        "without having access to the raw data. The example here runs everything locally; however there is only one parameter to\n",
        "change to run it on a real network.\n",
        "\n",
        "**Caution:**\n",
        " This example is provided as an illustrative example only. In real life, you should be careful not to\n",
        " accidentally leak private information when doing Federated Analytics. For example if a column contains very similar values,\n",
        " sharing its mean and its standard deviation is functionally equivalent to sharing the content of the column.\n",
        " It is **strongly recommended** to consider what are the potential security risks in your use case, and to act accordingly.\n",
        " It is possible to use other privacy-preserving techniques, such as\n",
        " [Differential Privacy](https://en.wikipedia.org/wiki/Differential_privacy), in addition to Substra.\n",
        " Because the focus of this example is Substra capabilities and for the sake of simplicity, such safeguards are not implemented here.\n",
        "\n",
        "\n",
        "To run this example, you need to download and unzip the assets needed to run it in the same directory as used this example.\n",
        "\n",
        "- [assets required to run this example](../../../tmp/diabetes_substrafl_assets.zip)\n",
        "\n",
        "Please ensure to have all the libraries installed. A *requirements.txt* file is included in the zip file, where you can run the command `pip install -r requirements.txt` to install them.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Instantiating the Substra clients\n",
        "\n",
        "We work with three different organizations.\n",
        "Two organizations provide data, and a third one performs Federated Analytics to compute aggregated statistics without\n",
        "having access to the raw datasets.\n",
        "\n",
        "This example runs in local mode, simulating a federated learning experiment.\n",
        "\n",
        "In the following code cell, we define the different organizations needed for our FL experiment.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from substra import Client\n",
        "\n",
        "# Choose the subprocess mode to locally simulate the FL process\n",
        "N_CLIENTS = 3\n",
        "client_0 = Client(client_name=\"org-1\")\n",
        "client_1 = Client(client_name=\"org-2\")\n",
        "client_2 = Client(client_name=\"org-3\")\n",
        "\n",
        "# Create a dictionary to easily access each client from its human-friendly id\n",
        "clients = {\n",
        "    client_0.organization_info().organization_id: client_0,\n",
        "    client_1.organization_info().organization_id: client_1,\n",
        "    client_2.organization_info().organization_id: client_2,\n",
        "}\n",
        "# Store organization IDs\n",
        "ORGS_ID = list(clients)\n",
        "\n",
        "# The provider of the functions for computing analytics is defined as the first organization.\n",
        "ANALYTICS_PROVIDER_ORG_ID = ORGS_ID[0]\n",
        "# Data providers orgs are the two last organizations.\n",
        "DATA_PROVIDER_ORGS_ID = ORGS_ID[1:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prepare the data\n",
        "\n",
        "The function `setup_diabetes` downloads if needed the *diabetes* dataset, and split it in two to simulate a\n",
        "federated setup. Each data organization has access to a chunk of the dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "\n",
        "from diabetes_substrafl_assets.dataset.diabetes_substrafl_dataset import setup_diabetes\n",
        "\n",
        "data_path = pathlib.Path.cwd() / \"tmp\" / \"data_diabetes\"\n",
        "data_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "setup_diabetes(data_path=data_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Registering data samples and dataset\n",
        "\n",
        "Every asset will be created in respect to predefined specifications previously imported from\n",
        "`substra.sdk.schemas`. To register assets, [Schemas](https://docs.substra.org/en/stable/documentation/references/sdk_schemas.html#schemas)\n",
        "are first instantiated and the specs are then registered, which generate the real assets.\n",
        "\n",
        "Permissions are defined when registering assets. In a nutshell:\n",
        "\n",
        "- Data cannot be seen once it's registered on the platform.\n",
        "- Metadata are visible by all the users of a network.\n",
        "- Permissions allow you to execute a function on a certain dataset.\n",
        "\n",
        "Next, we need to define the asset directory. You should have already downloaded the assets folder as stated above.\n",
        "\n",
        "A dataset represents the data in Substra. It contains some metadata and an *opener*, a script used to load the\n",
        "data from files into memory. You can find more details about datasets\n",
        "in the [API Reference DatasetSpec](https://docs.substra.org/en/stable/documentation/references/sdk_schemas.html#datasetspec).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from substra.sdk.schemas import DataSampleSpec\n",
        "from substra.sdk.schemas import DatasetSpec\n",
        "from substra.sdk.schemas import Permissions\n",
        "\n",
        "\n",
        "assets_directory = pathlib.Path.cwd() / \"diabetes_substrafl_assets\"\n",
        "assert assets_directory.is_dir(), \"\"\"Did not find the asset directory,\n",
        "a directory called 'assets' is expected in the same location as this file\"\"\"\n",
        "\n",
        "permissions_dataset = Permissions(public=False, authorized_ids=[ANALYTICS_PROVIDER_ORG_ID])\n",
        "\n",
        "dataset = DatasetSpec(\n",
        "    name=f\"Diabetes dataset\",\n",
        "    type=\"csv\",\n",
        "    data_opener=assets_directory / \"dataset\" / \"diabetes_substrafl_opener.py\",\n",
        "    description=data_path / \"description.md\",\n",
        "    permissions=permissions_dataset,\n",
        "    logs_permission=permissions_dataset,\n",
        ")\n",
        "\n",
        "# We register the dataset for each organization\n",
        "dataset_keys = {client_id: clients[client_id].add_dataset(dataset) for client_id in DATA_PROVIDER_ORGS_ID}\n",
        "\n",
        "for client_id, key in dataset_keys.items():\n",
        "    print(f\"Dataset key for {client_id}: {key}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The dataset object itself is an empty shell. Data samples are needed in order to add actual data.\n",
        "A data sample contains subfolders containing a single data file like a CSV and the key identifying\n",
        "the dataset it is linked to.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "datasample_keys = {\n",
        "    org_id: clients[org_id].add_data_sample(\n",
        "        DataSampleSpec(\n",
        "            data_manager_keys=[dataset_keys[org_id]],\n",
        "            path=data_path / f\"org_{i + 1}\",\n",
        "        ),\n",
        "        local=True,\n",
        "    )\n",
        "    for i, org_id in enumerate(DATA_PROVIDER_ORGS_ID)\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The flexibility of the ComputePlanBuilder class\n",
        "\n",
        "This example aims at explaining how to use the [Compute Plan Builder](https://docs.substra.org/en/stable/substrafl_doc/api/compute_plan_builder.html#compute-plan-builder)\n",
        "class, and how to use the full power of the flexibility it provides.\n",
        "\n",
        "Before starting, we need to have in mind that a federated computation can be represented as a graph of tasks.\n",
        "Some of these tasks need data to be executed (training tasks) and others are here to aggregate local results\n",
        "(aggregation tasks).\n",
        "\n",
        "Substra does not store an explicit definition of this graph; instead, it gives the user full flexibility to define\n",
        "the compute plan (or computation graph) they need, by linking a task to its parents.\n",
        "\n",
        "To create this graph of computations, SubstraFL provides the `Node` abstraction. A `Node`\n",
        "assigns to an organization (aka a Client) tasks of a given type. The type of the `Node` depends on the type of tasks\n",
        "we want to run on this organization (training or aggregation tasks).\n",
        "\n",
        "An organization (aka Client) without data can host an\n",
        "[Aggregation node](https://docs.substra.org/en/stable/substrafl_doc/api/nodes.html#aggregationnode).\n",
        "We will use the [Aggregation node](https://docs.substra.org/en/stable/substrafl_doc/api/nodes.html#aggregationnode) object to compute the aggregated\n",
        "analytics.\n",
        "\n",
        "An organization (aka a Client) containing the data samples can host a\n",
        "[Train data node](https://docs.substra.org/en/stable/substrafl_doc/api/nodes.html#traindatanode).\n",
        "Each node will only have access data from the organization hosting it.\n",
        "These data samples must be instantiated with the right permissions to be processed by the given Client.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from substrafl.nodes import TrainDataNode\n",
        "from substrafl.nodes import AggregationNode\n",
        "\n",
        "\n",
        "aggregation_node = AggregationNode(ANALYTICS_PROVIDER_ORG_ID)\n",
        "\n",
        "train_data_nodes = [\n",
        "    TrainDataNode(\n",
        "        organization_id=org_id,\n",
        "        data_manager_key=dataset_keys[org_id],\n",
        "        data_sample_keys=[datasample_keys[org_id]],\n",
        "    )\n",
        "    for org_id in DATA_PROVIDER_ORGS_ID\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The [Compute Plan Builder](https://docs.substra.org/en/stable/substrafl_doc/api/compute_plan_builder.html#compute-plan-builder) is an abstract class that asks the user to\n",
        "implement only three methods:\n",
        "\n",
        "- `build_compute_plan(...)`\n",
        "- `load_local_state(...)`\n",
        "- `save_local_state(...)`\n",
        "\n",
        "The `build_compute_plan` method is essential to create the graph of the compute plan that will be executed on\n",
        "Substra. Using the different `Nodes` we created, we will update their states by applying user defined methods.\n",
        "\n",
        "These methods are passed as argument to the `Node` using its `update_state` method.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "from typing import List, Dict\n",
        "\n",
        "from substrafl import ComputePlanBuilder\n",
        "from substrafl.remote import remote_data, remote\n",
        "\n",
        "\n",
        "class Analytics(ComputePlanBuilder):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.first_order_aggregated_state = {}\n",
        "        self.second_order_aggregated_state = {}\n",
        "\n",
        "    @remote_data\n",
        "    def local_first_order_computation(self, datasamples: pd.DataFrame, shared_state=None):\n",
        "        \"\"\"Compute from the data samples, expected to be a pandas dataframe,\n",
        "        the means and counts of each column of the data frame.\n",
        "        These datasamples are the output of the ``get_data`` function defined\n",
        "        in the ``diabetes_substrafl_opener.py`` file are available in the asset\n",
        "        folder downloaded at the beginning of the example.\n",
        "\n",
        "        The signature of a function decorated by @remote_data must contain\n",
        "        the datasamples and the shared_state arguments.\n",
        "\n",
        "        Args:\n",
        "            datasamples (pd.DataFrame): Pandas dataframe provided by the opener.\n",
        "            shared_state (None, optional): Unused here as this function only\n",
        "                use local information already present in the datasamples.\n",
        "                Defaults to None.\n",
        "\n",
        "        Returns:\n",
        "            dict: dictionary containing the local information on means, counts\n",
        "                and number of sample. This dict will be used as a state to be\n",
        "                shared to an AggregationNode in order to compute the aggregation\n",
        "                of the different analytics.\n",
        "        \"\"\"\n",
        "        df = datasamples\n",
        "        states = {\n",
        "            \"n_samples\": len(df),\n",
        "            \"means\": df.select_dtypes(include=np.number).sum().to_dict(),\n",
        "            \"counts\": {\n",
        "                name: series.value_counts().to_dict() for name, series in df.select_dtypes(include=\"category\").items()\n",
        "            },\n",
        "        }\n",
        "        return states\n",
        "\n",
        "    @remote_data\n",
        "    def local_second_order_computation(self, datasamples: pd.DataFrame, shared_state: Dict):\n",
        "        \"\"\"This function will use the output of the ``aggregation`` function to compute\n",
        "        locally the standard deviation of the different columns.\n",
        "\n",
        "        Args:\n",
        "            datasamples (pd.DataFrame): Pandas dataframe provided by the opener.\n",
        "            shared_state (Dict): Output of a first order analytics computation,\n",
        "                that must contain the means.\n",
        "\n",
        "        Returns:\n",
        "            Dict: dictionary containing the local information on standard deviation\n",
        "                and number of sample. This dict will be used as a state to be shared\n",
        "                to an AggregationNode in order to compute the aggregation of the\n",
        "                different analytics.\n",
        "        \"\"\"\n",
        "        df = datasamples\n",
        "        means = pd.Series(shared_state[\"means\"])\n",
        "        states = {\n",
        "            \"n_samples\": len(df),\n",
        "            \"std\": np.power(df.select_dtypes(include=np.number) - means, 2).sum(),\n",
        "        }\n",
        "        return states\n",
        "\n",
        "    @remote\n",
        "    def aggregation(self, shared_states: List[Dict]):\n",
        "        \"\"\"Aggregation function that receive a list on locally computed analytics in order to\n",
        "        aggregate them.\n",
        "        The aggregation will be a weighted average using \"n_samples\" as weight coefficient.\n",
        "\n",
        "        Args:\n",
        "            shared_states (List[Dict]): list of dictionaries containing a field \"n_samples\",\n",
        "            and the analytics to aggregate in separated fields.\n",
        "\n",
        "        Returns:\n",
        "            Dict: dictionary containing the aggregated analytics.\n",
        "        \"\"\"\n",
        "        total_len = 0\n",
        "        for state in shared_states:\n",
        "            total_len += state[\"n_samples\"]\n",
        "\n",
        "        aggregated_values = defaultdict(lambda: defaultdict(float))\n",
        "        for state in shared_states:\n",
        "            for analytics_name, col_dict in state.items():\n",
        "                if analytics_name == \"n_samples\":\n",
        "                    # already aggregated in total_len\n",
        "                    continue\n",
        "                for col_name, v in col_dict.items():\n",
        "                    if isinstance(v, dict):\n",
        "                        # this column is categorical and v is a dict over\n",
        "                        # the different modalities\n",
        "                        if not aggregated_values[analytics_name][col_name]:\n",
        "                            aggregated_values[analytics_name][col_name] = defaultdict(float)\n",
        "                        for modality, vv in v.items():\n",
        "                            aggregated_values[analytics_name][col_name][modality] += vv / total_len\n",
        "                    else:\n",
        "                        # this is a numerical column and v is numerical\n",
        "                        aggregated_values[analytics_name][col_name] += v / total_len\n",
        "\n",
        "        # transform default_dict to regular dict\n",
        "        aggregated_values = json.loads(json.dumps(aggregated_values))\n",
        "\n",
        "        return aggregated_values\n",
        "\n",
        "    def build_compute_plan(\n",
        "        self,\n",
        "        train_data_nodes: List[TrainDataNode],\n",
        "        aggregation_node: AggregationNode,\n",
        "        num_rounds=None,\n",
        "        evaluation_strategy=None,\n",
        "        clean_models=False,\n",
        "    ):\n",
        "        \"\"\"Method to build and link the different computations to execute with each other.\n",
        "        We will use the ``update_state``method of the nodes given as input to choose which\n",
        "        method to apply.\n",
        "        For our example, we will only use TrainDataNodes and AggregationNodes.\n",
        "\n",
        "        Args:\n",
        "            train_data_nodes (List[TrainDataNode]): Nodes linked to the data samples on which\n",
        "                to compute analytics.\n",
        "            aggregation_node (AggregationNode): Node on which to compute the aggregation\n",
        "                of the analytics extracted from the train_data_nodes.\n",
        "            num_rounds Optional[int]: Num rounds to be used to iterate on recurrent part of\n",
        "                the compute plan. Defaults to None.\n",
        "            evaluation_strategy Optional[substrafl.EvaluationStrategy]: Object storing the\n",
        "                TestDataNode. Unused in this example. Defaults to None.\n",
        "            clean_models (bool): Clean the intermediary models of this round on the\n",
        "                Substra platform. Default to False.\n",
        "        \"\"\"\n",
        "        first_order_shared_states = []\n",
        "        local_states = {}\n",
        "\n",
        "        for node in train_data_nodes:\n",
        "            # Call local_first_order_computation on each train data node\n",
        "            next_local_state, next_shared_state = node.update_states(\n",
        "                self.local_first_order_computation(\n",
        "                    node.data_sample_keys,\n",
        "                    shared_state=None,\n",
        "                    _algo_name=f\"Computing first order means with {self.__class__.__name__}\",\n",
        "                ),\n",
        "                local_state=None,\n",
        "                round_idx=0,\n",
        "                authorized_ids=set([node.organization_id]),\n",
        "                aggregation_id=aggregation_node.organization_id,\n",
        "                clean_models=False,\n",
        "            )\n",
        "\n",
        "            # All local analytics are stored in the first_order_shared_states,\n",
        "            # given as input the the aggregation method.\n",
        "            first_order_shared_states.append(next_shared_state)\n",
        "            local_states[node.organization_id] = next_local_state\n",
        "\n",
        "        # Call the aggregation method on the first_order_shared_states\n",
        "        self.first_order_aggregated_state = aggregation_node.update_states(\n",
        "            self.aggregation(\n",
        "                shared_states=first_order_shared_states,\n",
        "                _algo_name=\"Aggregating first order\",\n",
        "            ),\n",
        "            round_idx=0,\n",
        "            authorized_ids=set([train_data_node.organization_id for train_data_node in train_data_nodes]),\n",
        "            clean_models=False,\n",
        "        )\n",
        "\n",
        "        second_order_shared_states = []\n",
        "\n",
        "        for node in train_data_nodes:\n",
        "            # Call local_second_order_computation on each train data node\n",
        "            _, next_shared_state = node.update_states(\n",
        "                self.local_second_order_computation(\n",
        "                    node.data_sample_keys,\n",
        "                    shared_state=self.first_order_aggregated_state,\n",
        "                    _algo_name=f\"Computing second order analytics with {self.__class__.__name__}\",\n",
        "                ),\n",
        "                local_state=local_states[node.organization_id],\n",
        "                round_idx=1,\n",
        "                authorized_ids=set([node.organization_id]),\n",
        "                aggregation_id=aggregation_node.organization_id,\n",
        "                clean_models=False,\n",
        "            )\n",
        "\n",
        "            # All local analytics are stored in the second_order_shared_states,\n",
        "            # given as input the the aggregation method.\n",
        "            second_order_shared_states.append(next_shared_state)\n",
        "\n",
        "        # Call the aggregation method on the second_order_shared_states\n",
        "        self.second_order_aggregated_state = aggregation_node.update_states(\n",
        "            self.aggregation(\n",
        "                shared_states=second_order_shared_states,\n",
        "                _algo_name=\"Aggregating second order\",\n",
        "            ),\n",
        "            round_idx=1,\n",
        "            authorized_ids=set([train_data_node.organization_id for train_data_node in train_data_nodes]),\n",
        "            clean_models=False,\n",
        "        )\n",
        "\n",
        "    def save_local_state(self, path: pathlib.Path):\n",
        "        \"\"\"This function will save the important local state to retrieve after each new\n",
        "        call to a train or test task.\n",
        "\n",
        "        Args:\n",
        "            path (pathlib.Path): Path where to save the local_state. Provided internally by\n",
        "                Substra.\n",
        "        \"\"\"\n",
        "        state_to_save = {\n",
        "            \"first_order\": self.first_order_aggregated_state,\n",
        "            \"second_order\": self.second_order_aggregated_state,\n",
        "        }\n",
        "        with open(path, \"w\") as f:\n",
        "            json.dump(state_to_save, f)\n",
        "\n",
        "    def load_local_state(self, path: pathlib.Path):\n",
        "        \"\"\"Mirror function to load the local_state from a file saved using\n",
        "        ``save_local_state``.\n",
        "\n",
        "        Args:\n",
        "            path (pathlib.Path): Path where to load the local_state. Provided internally by\n",
        "                Substra.\n",
        "\n",
        "        Returns:\n",
        "            ComputePlanBuilder: return self with the updated local state.\n",
        "        \"\"\"\n",
        "        with open(path, \"r\") as f:\n",
        "            state_to_load = json.load(f)\n",
        "\n",
        "        self.first_order_aggregated_state = state_to_load[\"first_order\"]\n",
        "        self.second_order_aggregated_state = state_to_load[\"second_order\"]\n",
        "\n",
        "        return self"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we saw the implementation of the custom `Analytics` class, we can add details to some of the previously\n",
        "introduced concepts.\n",
        "\n",
        "The `update_state` method outputs the new state of the node, that can be passed as an argument to a following one.\n",
        "This succession of `next_state` passed to a new `node.update_state` is how Substra build the graph of the\n",
        "compute plan.\n",
        "\n",
        "The `load_local_state` and `save_local_state` are two methods used at each new iteration on a Node, in order to\n",
        "retrieve the previous local state that have not been shared with the other `Nodes`.\n",
        "\n",
        "For instance, after updating a [Train data node](https://docs.substra.org/en/stable/substrafl_doc/api/nodes.html#traindatanode) using its\n",
        "`update_state` method, we will have access to its next local state, that we will pass as argument to the\n",
        "next `update_state` we will apply on this [Train data node](https://docs.substra.org/en/stable/substrafl_doc/api/nodes.html#traindatanode).\n",
        "\n",
        "To summarize, a [Compute Plan Builder](https://docs.substra.org/en/stable/substrafl_doc/api/compute_plan_builder.html#compute-plan-builder) is composed of several decorated\n",
        "user defined functions, that can need some data (decorated with `@remote_data`) or not (decorated with `@remote`).\n",
        "\n",
        "See [Decorator](https://docs.substra.org/en/stable/substrafl_doc/api/remote.html#module-substrafl.remote.decorators) for more information on these decorators.\n",
        "\n",
        "These user defined functions will be used to create the graph of the compute plan through the `build_compute_plan`\n",
        "method and the `update_state` method of the different `Nodes`.\n",
        "\n",
        "The local state obtained after updating a [Train data node](https://docs.substra.org/en/stable/substrafl_doc/api/nodes.html#traindatanode) needs the\n",
        "methods `save_local_state` and `load_local_state`  to retrieve the state where the Node was at the end of\n",
        "the last update.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Running the experiment\n",
        "\n",
        "As a last step before launching our experiment, we need to specify the third parties dependencies required to run it.\n",
        "The [Dependency](https://docs.substra.org/en/stable/substrafl_doc/api/dependency.html#dependency) object is instantiated in order to install the right libraries in\n",
        "the Python environment of each organization.\n",
        "\n",
        "We now have all the necessary objects to launch our experiment. Please see a summary below of all the objects we created so far:\n",
        "\n",
        "- A [Client](https://docs.substra.org/en/stable/documentation/references/sdk.html#client) to add or retrieve the assets of our experiment, using their keys to\n",
        "  identify them.\n",
        "- A [Federated Strategy](https://docs.substra.org/en/stable/substrafl_doc/api/strategies.html#strategies), to specify what compute plan we want to execute.\n",
        "- [Train data nodes](https://docs.substra.org/en/stable/substrafl_doc/api/nodes.html#traindatanode) to indicate on which data to train.\n",
        "- An [Evaluation Strategy](https://docs.substra.org/en/stable/substrafl_doc/api/evaluation_strategy.html#evaluation-strategy), to define where and at which frequency we\n",
        "  evaluate the model. Here this does not apply to our experiment. We set it to None.\n",
        "- An [Aggregation Node](https://docs.substra.org/en/stable/substrafl_doc/api/nodes.html#aggregationnode), to specify the organization on which the aggregation operation\n",
        "  will be computed.\n",
        "- An **experiment folder** to save a summary of the operation made.\n",
        "- The [Dependency](https://docs.substra.org/en/stable/substrafl_doc/api/dependency.html#dependency) to define the libraries on which the experiment needs to run.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from substrafl.dependency import Dependency\n",
        "from substrafl.experiment import execute_experiment\n",
        "\n",
        "dependencies = Dependency(pypi_dependencies=[\"numpy==1.24.3\", \"pandas==1.5.3\"])\n",
        "\n",
        "compute_plan = execute_experiment(\n",
        "    client=clients[ANALYTICS_PROVIDER_ORG_ID],\n",
        "    strategy=Analytics(),\n",
        "    train_data_nodes=train_data_nodes,\n",
        "    evaluation_strategy=None,\n",
        "    aggregation_node=aggregation_node,\n",
        "    experiment_folder=str(pathlib.Path.cwd() / \"tmp\" / \"experiment_summaries\"),\n",
        "    dependencies=dependencies,\n",
        "    clean_models=False,\n",
        "    name=\"Federated Analytics with SubstraFL documentation example\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Results\n",
        "\n",
        "The output of a task can be downloaded using some utils function provided by SubstraFL, such as\n",
        "`download_algo_state`, `download_train_shared_state` or `download_aggregate_shared_state`.\n",
        "\n",
        "These functions download from a given `Client` and a given `compute_plan_key` the output of a\n",
        "given `round_idx` or `rank_idx`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from substrafl.model_loading import download_aggregate_shared_state\n",
        "\n",
        "# The aggregated analytics are computed in the ANALYTICS_PROVIDER_ORG_ID client.\n",
        "client_to_download_from = clients[ANALYTICS_PROVIDER_ORG_ID]\n",
        "\n",
        "# The results will be available once the compute plan is completed\n",
        "client_to_download_from.wait_compute_plan(compute_plan.key)\n",
        "\n",
        "first_rank_analytics = download_aggregate_shared_state(\n",
        "    client=client_to_download_from,\n",
        "    compute_plan_key=compute_plan.key,\n",
        "    round_idx=0,\n",
        ")\n",
        "\n",
        "second_rank_analytics = download_aggregate_shared_state(\n",
        "    client=client_to_download_from,\n",
        "    compute_plan_key=compute_plan.key,\n",
        "    round_idx=1,\n",
        ")\n",
        "\n",
        "print(\n",
        "    f\"\"\"Age mean: {first_rank_analytics['means']['age']:.2f} years\n",
        "Sex percentage:\n",
        "    Male: {100*first_rank_analytics['counts']['sex']['M']:.2f}%\n",
        "    Female: {100*first_rank_analytics['counts']['sex']['F']:.2f}%\n",
        "Blood pressure std: {second_rank_analytics[\"std\"][\"bp\"]:.2f} mm Hg\n",
        "\"\"\"\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
